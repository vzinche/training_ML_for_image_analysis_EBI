{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_script.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vzinche/training_ML_for_image_analysis_EBI/blob/master/training_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpEeR8v0U58r",
        "colab_type": "text"
      },
      "source": [
        "The first thing we have to do is to (unexpectedly!) take a look at the data.\n",
        "\n",
        "We are going to work with Kaggle 2018 Data Science Bowl data.\n",
        "To start with go the [data webpage](https://www.kaggle.com/c/data-science-bowl-2018), read the decsription and the evaluation parts (what do we have to do?), and then check the 'data' tab to see the data sctructure. \n",
        "\n",
        "As you can see only the 'stage1_train' part has ground truth available. Thus, we will to use it for training and evaluation. Additionally, we could test our model on 'stage1_test' or 'stage2_test', but this way we could judge the model performance only visually - we don't have the ground truth to get any numbers. \n",
        "\n",
        "To make things easier, let's start with downloading just the 'stage1_train' and 'stage1_test' data, that I stored on my Google Drive: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_8GkEjgM45l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O66UElt2ZfhLXUKKX_nTxmIXh6fMA2rT' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1O66UElt2ZfhLXUKKX_nTxmIXh6fMA2rT\" -O kaggle_data.zip && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUxpMoAVYDlK",
        "colab_type": "text"
      },
      "source": [
        "Remember that you can execute any bash command from the Notebook if you preceed the command name with '!'. \n",
        "\n",
        "And please check whether the downloaded archive is around 80M (the value after the progress bar [      <=>           ]). Sometimes Google fails :)\n",
        "\n",
        "Those of you who like bash can play around with unzipping the data into nice folders. The rest of you an just run the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcim5ub0Ytnt",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "!unzip -qq kaggle_data.zip && rm kaggle_data.zip\n",
        "!mkdir nuclei_train_data && unzip -qq stage1_train.zip -d nuclei_train_data/ && rm stage1_train.zip\n",
        "!mkdir nuclei_test_data && unzip -qq stage1_test.zip -d nuclei_test_data/ && rm stage1_test.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztgQ-O6KZN6I",
        "colab_type": "text"
      },
      "source": [
        "Don't forget to periodically proofread what's happening by listing the contents of your directory with '!ls'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeMLhYCfZnjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -ltrh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asZ3OxU2ZoTZ",
        "colab_type": "text"
      },
      "source": [
        "Now take a moment to think about the data you have. \n",
        "\n",
        "What will be the training data, the evaluation data and the testing data? How do you split it? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hir5ilYwo2pi",
        "colab_type": "text"
      },
      "source": [
        "Hint: you have ground truth only for 'stage1_train'. You might want to split it into the training and evaluation data. Normally, we would also want our test data to have some ground truth as well, since we would report the accuracy on the test data. In this case it's not necessary, since (if we would have participated in the actual Data Science Bowl) we would have gotten the accuracy value after submitting our solutions. \n",
        "\n",
        "Now think at hich stage you would split the data. Do you want to split it inot separate folders in bash or already in python?\n",
        "(You can just run the next cell to split it with bash)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1RKMt9JZCJd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "\n",
        "!mkdir nuclei_eval_data && n=0 && for file in nuclei_train_data/*; do test $n -eq 0 && mv \"$file\" nuclei_eval_data/; n=$((n+1)); n=$((n%4)); done\n",
        "!ls nuclei_train_data | wc -l && ls nuclei_test_data | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_OXk_ZWq_3f",
        "colab_type": "text"
      },
      "source": [
        "Now let's import the libraries we might need! I've added some, that are strickly necessary (in my opinion :), but you might need to add some more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD-2SXCuL_fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms, utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tioqg__rsJph",
        "colab_type": "text"
      },
      "source": [
        "What I would normally start with in any machine learning pipeline is writing a dataloader. Luckily most of the functionality is already provided by PyTorch, but what you need to do is to write a class, that will actually supply the dataloader with training samples - a Dataset.\n",
        "\n",
        "Please take a moment to read about it [here](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) and [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QiVEHw3_LsQ",
        "colab_type": "text"
      },
      "source": [
        "The main idea: any Dataset class should have two methods: \\_\\_len\\_\\_ that returns the dataset length (the number of element) and \\_\\_getitem\\_\\_ that, given an index, returns input (image) and target (ground truth)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIwZumG0D5AK",
        "colab_type": "text"
      },
      "source": [
        "Now try writing your own dataset. Think what you should include there (what do you want to do with your data?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut1sVinNEBNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jdaFyDIEI7t",
        "colab_type": "text"
      },
      "source": [
        "Otherwise, try making this already written prototype work (you should take care of all the TODO's)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCOLYPt1MT83",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "#any PyTorch dataset class should inherit the initial torch.utils.data.Dataset\n",
        "class NucleiDataset(Dataset):\n",
        "    \"\"\" A PyTorch dataset to load cell images and nuclei masks \"\"\"\n",
        "    def __init__(self, YOUR_ARGS, transform=None):     # which arguments would you need to build a dataset?\n",
        "        self.transform = transform    # we might want to apply some transformations to your data\n",
        "        self.YOUR_ARG = YOUR_ARG      # TODO save the variables that you pass as arguments \n",
        "        self.ANOTHER_ARG? = ANOTHER_ARG?\n",
        "        self.samples = # TODO we need to get a list of all the training samples\n",
        "        self.inp_transforms = transforms.Compose([transforms.Grayscale(),     # we'll have a mix of coloured and greyscale images, let's train a network on something consistent\n",
        "                                                  transforms.ToTensor(),      # all the data would have to be transformed to Torch tensors before training\n",
        "                                                  transforms.Normalize([0.5], [0.5])    # we would normally want to normalize our data\n",
        "                                                  ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = # TODO How would you get an image name given its id?\n",
        "        image = Image.open(img_name)    # we'll be ussing Pillow library for reading the files\n",
        "        image = image.convert(\"RGB\")    # this is just to adjust the axis order\n",
        "        image = self.inp_transforms(image)\n",
        "        masks_dir = # TODO What is the folder with the corresponding masks?\n",
        "        if not os.path.isdir(masks_dir):    # for the testing case, when we don't have the ground truth\n",
        "            return image\n",
        "        masks_list = os.listdir(masks_dir)\n",
        "        # if you haven't noticed already, each mask is stored in a separate image\n",
        "        # we have to iterate through all of them and sum them up\n",
        "        # TODO: your code here\n",
        "        if self.transform is not None:\n",
        "            image, mask = self.transform([image, mask])\n",
        "        return image, mask\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iFU9Lf_Vteo",
        "colab_type": "text"
      },
      "source": [
        "Once you are ready with your dataset class, let's load it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryWEN8LgM4yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATA_PATH = # insert the path here\n",
        "train_data = NucleiDataset(TRAIN_DATA_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmmS4VRKU8Ia",
        "colab_type": "text"
      },
      "source": [
        "And let's take this simple function to show the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz9Z0imPVObG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_dataset(dataset):\n",
        "    idx = np.random.randint(0, len(dataset))    # take a random sample\n",
        "    img, mask = dataset[idx]                    # get the image and the nuclei masks\n",
        "    f, axarr = plt.subplots(1, 2)               # make two plots on one figure\n",
        "    axarr[0].imshow(img[0])                     # show the image\n",
        "    axarr[1].imshow(mask[0])                    # show the masks\n",
        "    _ = [ax.axis('off') for ax in axarr]        # remove the axes\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwul0qJXNyRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_dataset(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbyjOq1GXWPl",
        "colab_type": "text"
      },
      "source": [
        "As you can probably see, if you clicked enough times, some of the images are really huge! What happens if we load them into memory and run the model on them? We might run out of memory. That's why normally, when training networks on images or volumes one has to be really careful about the sizes. In practice, you would want to regulate their size. Additional reason for restraining the size is: if we want to train in batches (faster and more stable training), we need all the images in the batch to be of the same size. That is why we prefer to either resize or crop them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZZR1c5paaXJ",
        "colab_type": "text"
      },
      "source": [
        "Here is a function (well, actually a class), that will apply a transformation 'random crop'. Notice that we apply it to images and masks simultaneously to make sure they correspond, despite the randomness.\n",
        "\n",
        "In case anybody is wondering why we have to bother to write a whole class for it instead of simply coping the images directly in the dataset: we want to keep the code modular. We want to write one dataset object, and then we can try all the possible transforms with this one dataset. Similarly, we want to write one Randomcrop transform object, and then we can reuse it for any other image datasets we night have in the future. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7g2msUEYmx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the input image and the output mask\"\"\"\n",
        "    def __init__(self, crop_size):\n",
        "        assert isinstance(crop_size, (int, tuple, list))\n",
        "        if isinstance(crop_size, int):\n",
        "            self.output_size = (crop_size, crop_size)\n",
        "        else:\n",
        "            assert len(crop_size) == 2\n",
        "            self.crop_size = crop_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        assert len(sample) == 2\n",
        "        image, mask = sample\n",
        "        w, h = image.shape[1:]\n",
        "        new_w, new_h = self.output_size\n",
        "        top = np.random.randint(0, h - new_h) if h - new_h > 0 else 0\n",
        "        left = np.random.randint(0, w - new_w) if w - new_w > 0 else 0\n",
        "        image = image[:, left: left + new_w, top: top + new_h]\n",
        "        mask = mask[:, left: left + new_w, top: top + new_h]\n",
        "        return image, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVvofiataVqK",
        "colab_type": "text"
      },
      "source": [
        "PS: PyTorch already has quite a bunch of all possible data transforms, so if you need one, check [here](https://pytorch.org/docs/stable/torchvision/transforms.html). The biggest problem with them is that they are clearly separated into transforms applied to PIL images (_remember, we initially load the images as PIL.Image?_) and Torch tensors (_remember, we converted the images into tensors by calling transforms.ToTensor()?_). This can be incredibly annoying if for some reason you might need to transorm your images to tensors before applying any other transforms or you don't want to use PIL library at all. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vzNe8SYdGyg",
        "colab_type": "text"
      },
      "source": [
        "Now let's get a new dataset with cropping and check it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D55pJ44dQF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = NucleiDataset(TRAIN_DATA_PATH, RandomCrop(256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRN8dCurdeV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_dataset(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2I2nrYXdh8Z",
        "colab_type": "text"
      },
      "source": [
        "And the same for the evaluation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIozWvPXj2_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EVAL_DATA_PATH = # insert your path here\n",
        "eval_data = NucleiDataset(EVAL_DATA_PATH, RandomCrop(256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmkDq9ondpfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_dataset(eval_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl9dJQK3c698",
        "colab_type": "text"
      },
      "source": [
        "Now comes a harder part :D "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd34zcidEvHg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We need to define the architecture of the model to use. I suggest a [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) that has proven to steadily outperform the other architectures in segmenting biological and medical images.\n",
        "\n",
        "The image of the model precisely describes all the building blocks you need to use to create it. All of them can be found in the list of PyTorch layers (modules) [here](https://pytorch.org/docs/stable/nn.html#convolution-layers).\n",
        "\n",
        "So if you feel like it, you can try to construct the model yourself. If not, luckily for you, from my personal experience: whatever new you want to try out in the Deep Learning field, if it has been published, most likely you will find the code somewhere in the internet (GitHub!). So feel free to google 'U-Net pytorch' and see what you find. You still have to read through the code you've found to make sure it makes sense and doesn't use any weird libraries that we don't want to install here.\n",
        "\n",
        "__Additional note__: even if you're able to write the model/layer/loss/whatever yourself, it makes more sense to first look for it in the internet, because it is pretty likely that you can simply find something better implemented (more efficient, numerically stable, etc). But don't foget to cite!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ubfbQTMbmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, ARGS):     # you start with initializing the model with needed parameters\n",
        "        #TODO: your code here\n",
        "\n",
        "    def forward(self, x):         # this is the function that will actually take your input (images) and process them to generate the output (segmentation)\n",
        "        #TODO: your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_VDU1x_3dEm",
        "colab_type": "text"
      },
      "source": [
        "The next step to do would be writing a loss function - a metric that will tell us how close we are to the desired output. This metric should be differentiable, since this is the value to be backpropagated. The are [multiple losses](https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/) we could use.\n",
        "\n",
        "Take a moment to think which one is better to use. If you are not sure, don't forget that you can always _google_!\n",
        "Before you start implemeting the loss yourself, take a look at losses already implemented in [PyTorch](https://pytorch.org/docs/stable/nn.html#loss-functions). You can also look for implementatons on GitHub. \n",
        "\n",
        "TODO: find what a Focal loss is. Try to explain the rationale behind using it. Try implementing it yourself or find some satisfactory implementation on GitHib.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf9EFN5UOfnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO : your focal (or some other) loss here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiiSIarJOwZG",
        "colab_type": "text"
      },
      "source": [
        "Let's also define a helper function that will calculate [Intersection Over Union](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) for us. Why do we need it? Because that is the metric that will be used to evaluate our submissions (check the 'evaluation' tab on the dataset website). Why we don't use it as a loss directly? It's not differentiable and has 'not the best' surface properties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPrn-YxeMXzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_iou(prediction, mask):\n",
        "    iou = (prediction & mask).sum().float() / (prediction | mask).sum().float()\n",
        "    return iou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuGDMqUZSE1b",
        "colab_type": "text"
      },
      "source": [
        "Now let's write a function that will take care of all the training steps, namely, for every training sample (or training batch), take a forward step, compute the loss, backpropagate, update. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hnWETz3kF1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, test_loader, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # print a nice progress bar\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()   # set the model to the training state\n",
        "\n",
        "        #TODO: initialize any values you want to track (loss, accuracy, iou?)\n",
        "\n",
        "        for images, masks in iterate(train_loader):\n",
        "            optimizer.zero_grad()    # erase all the gradient from the previous steps\n",
        "            outputs = model(images)    # predict\n",
        "            predictions = (outputs > 0.5)    # binarize the predictions to get a mask\n",
        "\n",
        "            # you might want to be able to visually judge how the training is going\n",
        "            # use the show_dataset function as a template to write a function\n",
        "            # that will display input images, ground truth masks and the model predeictions\n",
        "            # use it to show the progress, let's say, every 10th iteration\n",
        "            # TODO: write show_images function and apply it every 10th iteration \n",
        "\n",
        "            # TODO: apply the loss (you need the output and the ground truth)\n",
        "            loss = \n",
        "\n",
        "            # TODO: calculate the pixelwise accuracy (just take the mean of predictions==target)\n",
        "            accuracy = \n",
        "\n",
        "            # TODO: calculate the IoU (use the function you wrote before)\n",
        "            iou = \n",
        "\n",
        "            loss.backward()    # compute the gradients for every neuron\n",
        "            optimizer.step()    # backpropagate!\n",
        "\n",
        "            # TODO: update the values you are tracking\n",
        "\n",
        "        # TODO: for every epoch print the values that we are tracking\n",
        "        # Remember that we interested in numbers per sample, not per whole dataset \n",
        "        # (e.g., accuracy=53.6 will not tell you anything meaningful)\n",
        "\n",
        "        # every epoch we want to check how the model performs on a previously unseen data\n",
        "        # for this we need to run the model on our evaluation data and check the values\n",
        "        # For testing purposes you would want to have evaluation state of you model - 'model.eval()'\n",
        "        # Moreover, you wouldn't want to calculate gradients or backpropagate now\n",
        "        # So in order to save time you would run all the iterations with - 'with torch.no_grad():'\n",
        "        # TODO: write the evaluation\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEuoCqgXkXpq",
        "colab_type": "text"
      },
      "source": [
        "Now when we have the main train function, let's prepare all we need for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flWvk4BNkAP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the actual PyTorch loaders!\n",
        "train_dataloader = DataLoader(train_data, batch_size=5, shuffle=True) # you can adjust the batch size to fit your memory\n",
        "eval_dataloader = DataLoader(eval_data, batch_size=5)\n",
        "# create a model\n",
        "model = UNet(PARAMETERS) # TODO: which parameters do you need to initialize your UNet?\n",
        "# and an optimizer - it will take care of updating our parameters properly\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQMGrNh9k1r_",
        "colab_type": "text"
      },
      "source": [
        "Take a moment to [read](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/) about the importance of setting a proper learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXgPlnLmlK52",
        "colab_type": "text"
      },
      "source": [
        "Now we're good to go. Let's see how the training goes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1AGBjXxkILn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = train(model, optimizer, train_dataloader, eval_dataloader, num_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCh5NaE1UzEA",
        "colab_type": "text"
      },
      "source": [
        "The first thing you might notice is that the training is moving incredibly slow. That's because we have quite some heavy computations to run. A thing to consider once you have an idea you want to use Deep Learning for image processing: you might want to train on a GPU!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zScxYhTqWtAr",
        "colab_type": "text"
      },
      "source": [
        "Luckily Google Colab Notebooks have GPU's available. For this you need to go to Edit -> Notebook Settings -> Hardware accelerator and choose GPU. Note: this will restart the whole notebook and clean your home directory. You would have to rerun all the cells, including the ones where you download and unzip the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmtpcLWBXe4I",
        "colab_type": "text"
      },
      "source": [
        "Now we'll have to tell PyTorch to train on GPUs. Firstly, let's confirm it's available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpfQluaDUWpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")   # check if we have GPUs\n",
        "print(device)   # this should print 'cuda:0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5gvRsm_X-io",
        "colab_type": "text"
      },
      "source": [
        "What we have to do now is to bring all the data we need for traing to the device we use for training. It is done by calling the method to() on you data, for instance:\n",
        "\n",
        "`model = model.to(device)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9eyooUVYeZn",
        "colab_type": "text"
      },
      "source": [
        "What I suggest is to add device as an argument for your function, and then transfer everything to this device. Then you can just set this device to gpu or cpu while calling the train function.\n",
        "\n",
        "Hint: you would want to send all the inputs and targets, as well as the model to GPU. For some postprossesing (e.g. visualising images) you would need to send it back to cpu by calling `data.cpu()`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioaBAfr3ZnQ9",
        "colab_type": "text"
      },
      "source": [
        "TODO: rerun the same training on GPU. Enjoy the speed improvement! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXrRdNikb0QZ",
        "colab_type": "text"
      },
      "source": [
        "The next step (I naively assume we still have time) is to implement a nicer way to track the progress of our training, than simply printing all the metrics every epoch. For this we would want to use an amazing tool called [tensorboard](https://www.tensorflow.org/tensorboard). It is developed by TensorFlow, but can be integrated with PyTorch as well. \n",
        "\n",
        "There is an amazing tutorial on what can be visualised with TensorBoard in PyTorch [here](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html). For now we are just interested in tracking our metrics (scalars). For this we just need to create [a summary writer](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html#tensorboard-setup) and [add our scalars there](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html#tracking-model-training-with-tensorboard).\n",
        "\n",
        "Adding scalars is as simple as calling this function:\n",
        "\n",
        "`writer.add_scalar(scalar_name, scalar_value, step_number)` \n",
        "\n",
        "where the step number is (epoch number * dataset size + iteration count)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45kklMxqjTtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: modify your train function to add your metrics (loss, accuracy and iou) to the writer every epoch \n",
        "# HINT: you might want to add the writer as an argument to your train function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb6NevRXoxX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: now initialize your summary writer (mind the directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b28QHCLLrBTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir YOUR_DIR_HERE    # launch tensorboard from inside the notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOSf1Gp4qRGu",
        "colab_type": "text"
      },
      "source": [
        "Now run the training and enjoy tracking the metrics!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6-4tWY1jWwb",
        "colab_type": "text"
      },
      "source": [
        "For people who have time and want to play around: try to visualize the graph (model) that we are using with TensorBoard."
      ]
    }
  ]
}